# Dynamo DB Notes:
- Sacrificing consistency for availability - eventual consistency
- Highly reliable/scalable KV store. ACID data stores have poor availability, and traditional replicated RDBMS systems focus on guaranteeing strong consistency to replicated data. Hence, they are limited in scalability and reliability.
- Decentralized, loosely coupled, hundreds of services.
- Customers should always be able top view and add items to shopping cart, so service always has to be able to write to and read from data store + data needs to be available across data centers.
- Consistent hashing to partition and replicate data, consistency facilitated by obj versioning.
- Quorums + decentralized replica synchronization protocol for consistency among replicas.
- Gossip based failure detection
- Most microservices @ Amazon retrieve data by primary key, and do not need querying of RDBMS
- Query Model: State is stored as binary objects (blobs) identified by unique keys. No isolation guarantees, single key updates.
- Clients and services engage in SLAs (agreement between client + service, client’s expected request rate distribution for an API + service latency).
    - Ex: Service guaranteeing to provide a response within 300ms for 99.9% of requests for peak client load of 500 requests per second
- Services are stateless
- Data replication algorithms in commercial systems perform synchronous replica coordination to provide a strongly consistent data access interface. These are forced to tradeoff the availability of data.
- When dealing with network failures, strong consistency and high availability cannot be achieved simultaneously.
- Dynamo ⇒ always writeable data store.
    - conflict resolution is pushed to reads during server failures, to ensure writes are never rejected.
- Last write wins methodology during conflicts
- Incremental scalability: should be able to scale out one storage host(node) at a time with minimal impact on operators of the system and system itself
- Symmetry: Every node in Dynamo should have the same set of responsibilities as its peers.
- Decentralization: Favors peer-to-peer techniques over centralized control (something like using Zookeeper). More scalable/available
- Heterogeneity: work distribution must be proportional to capabilities of individual servers
- Structured P2P networks use a globally consistent protocol to ensure any node can efficiently route a search query to some peer that has the desired data. Routing mechanisms ensure queries can be answered within a bounded # of hops.
    - To reduce latency from multi-hop routing, some P2P systems use O(1) routing where each peer maintains enough routing info locally to route requests to appropriate peer within constant # of hops.
- Distributed file systems support hierarchical namespaces. GFS uses a single master server for hosting metadata, data is split into chunks and stored in chunk servers.
- Similarly, Dynamo allows read and write operations to continue during network.
    - KV store is intended to store smaller obj (size < 1M)
    - easier to configure on per-app basis.
- Dynamo does not focus on problem of data integrity, and is built for a trusted env, unlike Antiquity (distributed storage system) which uses a secure log to preserve data integrity, and replicates each log on multiple servers, and uses Byzantine fault tolerance protocols to ensure consistency.
- BigTable has a sparse multi-dimensional sorted map to allow apps to access data using multiple attributes. Conversely, dynamo targets apps that require only K/V access with a focus on high availability.
- 4 rules:
    - always writeable
    - all nodes assumed to be trusted
    - apps that use dynamo do not require support for hierarchical namespaces
    - built for latency sensitive apps that require 99.9% of read and write operations to be performed within a few milliseconds.
- To meet these latency requirements, routing requests thru multiple nodes should be avoided. Dynamo uses zero-hop DHT, where each node maintains enough info locally to route a request to appropriate node.
- system needs to have scalable and robust solutions for load balancing, membership and failure detection, failure recovery, replica synchronization, overload handling, state transfer, concurrency and job scheduling, request marshalling, request routing, system monitoring and alarming, and configuration management.
- core distributed systems techniques used in Dynamo: partitioning, replication, versioning, membership, failure handling and scaling.
- Utilizes a key-value interface for object storage, supporting **`get()`** and **`put()`** operations, employs MD5 hashing for key-based data placement and replication, and manages data versioning and validity through contextual metadata.
- To scale incrementally, data must be **partitioned** over set of nodes.
    - Consistent hashing to distribute load across multiple storage hosts:
        - output range of hash function treated as fixed circular ring
        - each node assigned random value, representing position
        - each data item identified by key is assigned to node, by hashing data item’s key to get position on ring + walking rings clockwise to find first node with larger position than cur.
        - each node becomes responsible for the region in the ring between it and predecessor node on the ring.
            - departure or arrival of node only affects immediate neighbors and other nodes remain unaffected.
        - Dynamo addresses consistent hashing challenges by introducing virtual nodes, enabling uniform data distribution and load balancing
            - instead of mapping a node to a single point in the circle, each node gets assigned to multiple points in the ring
            - if a node becomes unavailable, load handled by this node is evenly dispersed across remaining available nodes.
            - when node becomes available again or a new node is added to the system, the newly available node accepts an equivalent amount of load from other available nodes.
- Dynamo **replicates** data on multiple hosts:
    - Data is replicated across N hosts, with each key assigned to a coordinator node for replication to N-1 clockwise successor nodes, ensuring each node covers a segment of the ring and maintains a "preference list" of nodes responsible for each key.
    - The preference list extends beyond N to compensate for node failures and virtual nodes, ensuring it includes distinct physical nodes by skipping over successive virtual positions owned by the same physical node to maintain diversity in data storage responsibility.
- Dynamo provides eventual consistency which allows for updates to be propagated to all replicas asynchronously.
    - Dynamo allows applications like shopping carts (adding items to cart and deleting are put requests) to function with eventual consistency by maintaining immutable versions of data, enabling both concurrent updates and reconciliation of divergent data versions to ensure no operation (add/remove) is lost despite failures or network issues.
    - Certain failure modes can result in system having several versions of the same data. Must design apps that acknowledge the possibility of multiple versions of the same data.
    - Dynamo uses vector clocks ****to ****capture causality between different versions of the same object.
        - A vector clock is a list of (node, counter) pairs. One vector clock is associated with every version of every object. Can determine whether two versions of an object are on parallel branches or have casual ordering by observing vector clocks.
        - If counters on 1st obj’s clock ≤ all nodes of 2nd clock, then 1st is an ancestor of 2nd, and can't be forgotten. Otherwise, the two changes are a conflict and require reconciliation.
    - A possible issue with vector clocks is that the size of vector clocks may grow if many servers coordinate the writes to an object.
        - Not likely because writes are handled by one of the top N nodes in preference list.
        - In case of network partitions or multiple server failures, write requests may be handled by nodes that are not in the top N nodes in preference list, causing size of vector clock to grow.
            - Desirable to limit size of vector clock here.
            - Truncation scheme:
                - Along with each (node, counter) pair, store a timestamp that indicates the last time the node updated the data item. When the number of pairs in vector clock reaches threshold, oldest pair is removed from clock.
                - can lead to inefficiencies in reconciliation.
- Any storage in Dynamo can receive client get and put.
    - Invoked using AWS’s request processing over HTTP. Two strategies a client can use to to select node:
        - Route request through load balancer that will select node based on load info. Client does not have to link any code specific to Dynamo in app
        - partition aware client that routes requests directly to appropriate coordinator nodes. Lower latency as we’re skipping forwarding step.
    - Node handling read or write ops is coordinator.
        - First among top N nodes in preference list
        - If requests are received through load balancer, requests to access a key may be routed to any random node in ring.
    - Read and write ours involve first N healthy nodes in preference list, skipping over those that are down or inaccessible. When all nodes are healthy, the top N nodes in a key’s preference list are accessed. During node failures or network partitions, nodes that are lower ranked in preference list are accessed.
- To maintain consistency among replicas, Dynamo uses a quorum based consistency protocol.
    - To key config Vals: R, W. R = min number of nodes that must participate in successful read operation. W =  min number of nodes that must participate in write operation.
    - R + W > N yields quorum like system - latency of get or put operation is dictated by slowest of R or W replicas. Due to this, R and W are configured to be < N.
    - Upon receiving put request for key, coordinator generates vector clock for new version and writes new version locally. coordinator then sends new version to N highest ranked reachable nodes. If at least W-1 nodes respond, write = successful.
    - Similarly for get request, coordinator requests all existing versions of data for that key from N highest ranked reachable nodes in preference list for the key, and waits for R responses before returning result to client. If multiple versions of data gathered, all are returned.
- If Dynamo used a strict quorum membership scheme, it would be unavailable during server failures and network partitions.
    - Dynamo utilizes a sloppy quorum protocol. All read and write operations are performed on the first N healthy nodes from the preference list, which may not always be the first N nodes encountered while walking the consistent hashing ring.
    - Using hinted handoff, Dynamo ensures that the read and write operations are not failed due to temporary node or network failures.
    - Ex:
        - If node A is temporarily down during a write operation, then a replica that would normally have lived on A will now be sent to node D. This replica (sent to D) will have a hint in its metadata that suggests which node was the intended recipient of the replica (A).
        - Once A is recovered, D may delete the object from its local store without decreasing the total number of replicas in the system.
    - Hinted handoff allows Dynamo to hurdle past node/network failures.
    - Applications that need the highest level of availability can set W to 1, which ensures that a write is accepted as long as a single node in the system has durably written the key it to its local store. Thus, the write request is only rejected if all nodes in the system are unavailable.
- Hinted handoff works best when system membership churn is low.  To detect inconsistencies between replicas and minimize the amt. of transferred data, Dynamo uses Merkle Trees.
    - A Merkle tree is a hash tree where leaves are hashes of the values of individual keys. Parent nodes higher in the tree are hashes of their respective children.
    - Advantage is that each branch of the tree can be checked independently without requiring nodes to download the entire tree of the entire dataset. Merkle trees help in reducing the amount of data that needs to be transferred while checking for inconsistencies among replicas.
        - E.g  if the hash values of the root of two trees are equal, then the values of the leaf nodes in the tree are equal and the nodes require no synchronization.
        - f not, it implies that the values of some replicas are different. In such cases, the nodes may exchange the hash values of children and the process continues until it reaches the leaves of the trees, at which point the hosts can identify the keys that are “out of sync”.
        - Merkle trees minimize the amount of data that needs to be transferred for synchronization and reduce the number of disk reads performed during the anti-entropy process.
    - In Dynamo, each node maintains a separate Merkle tree for each key range (the set of keys covered by a virtual node) it hosts. Allows nodes to compare whether the keys within a key range are up-to-date.
- A node outage should not result in rebalancing of partition assignment or repair of unreachable replicas. manual error could result in unintentional startup of new Dynamo nodes.
    - Dynamo's architecture includes an explicit mechanism for node management, where administrators can add or remove nodes using a command line tool or a browser interface, initiating these changes directly on a Dynamo node.
    - Membership changes in the Dynamo system are recorded persistently and propagate through a gossip-based protocol, ensuring all nodes maintain an eventually consistent view of the network's composition.
    - Upon initialization, each Dynamo node selects its virtual nodes (tokens) within the consistent hash space, and these mappings are stored and regularly reconciled across nodes through the same gossip-based communications that manage membership changes. This mechanism ensures that each node knows which peers to contact for specific key operations.
    - This could result in a logically partitioned Dynamo ring, e.g. admin could contact node A to join A to the the ring, the contact node B to join B to the ring. A and B would each consider itself a member of the ring, yet neither would be aware of the other.
        - to prevent this, dynamo nodes play the role of seeds:
            - seeds are nodes that are discovered via an external mechanism and are known to all nodes. Because all nodes eventually reconcile their membership with a seed, logical partitions are highly unlikely.
- Failure detection in Dynamo is used to avoid attempts to communicate with unreachable peers during get() and put() operations, and when transferring partitions and hinted replicas.
    - For avoiding failed attempts at communication, a local notion of failure detection is sufficient: node A may consider node B failed if node B does not respond to node A’s messages (even if B is responsive to node C’s messages).
    - Node A then uses alternate nodes to service requests that map to B’s partitions. A periodically retries B to check for recovery.
    - In the absence of client requests to drive traffic between two nodes, neither node really needs to know whether the other is reachable and responsive.
    - Decentralized failure detection protocols use a simple gossip-style protocol that enable each node in the system to learn about the arrival (or departure) of other nodes.
- When a new node is added into the system, it gets assigned a number of tokens that are randomly scattered on the ring.
    - For every key range that is assigned to node X, there may be a number of nodes (less than or equal to N) that are currently in charge of handling keys that fall within its token range.
    - Due to the allocation of key ranges to X, some existing nodes no longer have to some of their keys and these nodes transfer those keys to X.
- In Dynamo, each storage node has three main software components: request coordination, membership and failure detection, and a local persistence engine.
